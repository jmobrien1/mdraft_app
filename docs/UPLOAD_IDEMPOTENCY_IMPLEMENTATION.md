# Upload Idempotency Implementation

## Overview

This document describes the implementation of atomic and idempotent uploadâ†’job creation under concurrency. The goal is to ensure that multiple concurrent uploads of the same file create only one conversion job, while maintaining high performance and reliability.

## Problem Statement

The original upload handler had a race condition where multiple concurrent requests could:
1. Check for existing conversions simultaneously
2. Find no existing conversion (because none exists yet)
3. Create multiple conversion records for the same file
4. Result in duplicate processing and wasted resources

## Solution Architecture

### 1. Database Constraints

**Unique Constraint**: `(sha256, user_id NULLS FIRST, visitor_session_id NULLS FIRST)`
- Ensures only one conversion per file per owner
- Handles both authenticated users (`user_id`) and anonymous users (`visitor_session_id`)
- Uses `NULLS FIRST` to handle cases where one field is NULL

**Indexes**:
- `(status, user_id)` - For efficient lookups by status and user
- `(status, visitor_session_id)` - For efficient lookups by status and visitor session

### 2. Atomic Upload Handler

The `_atomic_upload_handler` function implements the core idempotency logic:

```python
def _atomic_upload_handler(file_hash, filename, original_mime, original_size, 
                          gcs_uri, owner_fields, ttl_days, callback_url=None):
    # 1. Begin transaction
    db.session.begin()
    
    try:
        # 2. SELECT ... FOR UPDATE to lock existing conversion
        result = db.session.execute(
            text("SELECT id, status, filename, markdown FROM conversions WHERE ... FOR UPDATE"),
            params
        ).fetchone()
        
        if result:
            # 3. Return existing conversion if found
            return existing_conversion_response()
        
        # 4. Create new conversion and enqueue task
        conv = Conversion(...)
        db.session.add(conv)
        db.session.flush()
        task_id = enqueue_conversion_task(...)
        db.session.commit()
        
        return new_conversion_response()
        
    except Exception:
        db.session.rollback()
        raise
```

### 3. Concurrency Handling

**SELECT ... FOR UPDATE**: Locks any existing row with the same SHA256+owner combination, preventing race conditions.

**Transaction Isolation**: The entire operation (check + create) happens within a single database transaction.

**Error Handling**: Proper rollback on errors ensures database consistency.

## Implementation Details

### Migration: `add_conversion_idempotency_constraints.py`

```sql
-- PostgreSQL
ALTER TABLE conversions 
ADD CONSTRAINT uq_conversions_sha256_owner 
UNIQUE (sha256, user_id NULLS FIRST, visitor_session_id NULLS FIRST)
WHERE sha256 IS NOT NULL;

CREATE INDEX ix_conversions_status_user_id 
ON conversions (status, user_id) 
WHERE user_id IS NOT NULL;

CREATE INDEX ix_conversions_status_visitor_id 
ON conversions (status, visitor_session_id) 
WHERE visitor_session_id IS NOT NULL;
```

### Model Updates: `app/models_conversion.py`

```python
class Conversion(db.Model):
    # ... existing fields ...
    
    __table_args__ = (
        UniqueConstraint(
            'sha256', 'user_id', 'visitor_session_id', 
            name='uq_conversions_sha256_owner'
        ),
        Index('ix_conversions_status_user_id', 'status', 'user_id'),
        Index('ix_conversions_status_visitor_id', 'status', 'visitor_session_id'),
    )
```

### API Handler: `app/api_convert.py`

The main upload endpoint now:
1. Calculates SHA256 for idempotency
2. Uses atomic handler for normal uploads
3. Falls back to legacy handler for `force=true` parameter
4. Maintains backward compatibility

## Testing

### Unit Tests: `tests/test_upload_idempotency.py`

Comprehensive test suite covering:
- Single upload creation
- Duplicate upload detection
- Completed conversion reuse
- Concurrent upload handling
- Database constraint validation
- Mixed user/visitor scenarios
- Error handling and rollback

### Integration Test: `scripts/test_concurrent_uploads.py`

Simple script to validate concurrent upload behavior:
```bash
python scripts/test_concurrent_uploads.py http://localhost:5000
```

## Performance Considerations

### Database Performance
- **Indexes**: Optimize queries for status-based filtering
- **Partial Indexes**: PostgreSQL partial indexes reduce index size
- **Unique Constraints**: Prevent duplicate processing at database level

### Application Performance
- **Early Returns**: Completed conversions returned immediately
- **Minimal Locking**: Only lock during the critical section
- **Efficient Queries**: Use indexed columns for lookups

## Backward Compatibility

### Force Parameter
The `force=true` parameter bypasses atomic handling and uses the legacy upload path, maintaining compatibility with existing clients that expect this behavior.

### API Response Format
Response format remains unchanged, with additional `note` field indicating the reason for the response:
- `"deduplicated"` - Existing completed conversion returned
- `"duplicate_upload"` - Existing pending conversion returned
- `"duplicate_detected"` - IntegrityError handled gracefully

## Monitoring and Observability

### Logging
- Idempotency hits are logged with SHA256 prefix
- Duplicate uploads are logged with conversion ID
- Error conditions are logged with full context

### Metrics
Key metrics to monitor:
- Idempotency hit rate
- Duplicate upload rate
- Transaction rollback rate
- Upload latency

## Deployment Considerations

### Migration Safety
1. **Backup**: Always backup database before running migration
2. **Downtime**: Migration can be run with minimal downtime
3. **Rollback**: Migration includes downgrade path
4. **Testing**: Test migration on staging environment first

### Configuration
- **Database**: Ensure PostgreSQL for optimal performance
- **Connection Pooling**: Configure appropriate pool sizes
- **Timeout Settings**: Adjust transaction timeouts if needed

## Future Enhancements

### Potential Improvements
1. **Caching**: Redis cache for frequently accessed conversions
2. **Batch Processing**: Batch similar uploads for efficiency
3. **Metrics**: Prometheus metrics for monitoring
4. **Alerting**: Alerts for high duplicate rates

### Scalability Considerations
1. **Sharding**: Database sharding for high-volume deployments
2. **Read Replicas**: Separate read/write operations
3. **CDN**: Edge caching for completed conversions

## Troubleshooting

### Common Issues

**IntegrityError on Migration**
- Check for existing duplicate data
- Clean up duplicates before running migration

**High Lock Contention**
- Monitor database lock wait times
- Consider connection pool tuning

**Performance Degradation**
- Check index usage with `EXPLAIN ANALYZE`
- Monitor query performance metrics

### Debug Commands

```sql
-- Check for duplicate conversions
SELECT sha256, user_id, visitor_session_id, COUNT(*) 
FROM conversions 
WHERE sha256 IS NOT NULL 
GROUP BY sha256, user_id, visitor_session_id 
HAVING COUNT(*) > 1;

-- Check index usage
SELECT schemaname, tablename, indexname, idx_scan, idx_tup_read, idx_tup_fetch
FROM pg_stat_user_indexes 
WHERE tablename = 'conversions';
```

## Conclusion

This implementation provides robust, scalable idempotent upload handling that:
- Prevents duplicate conversions under any concurrency scenario
- Maintains high performance with proper indexing
- Preserves backward compatibility
- Includes comprehensive testing and monitoring
- Supports both authenticated and anonymous users

The solution is production-ready and handles edge cases gracefully while providing clear observability into system behavior.
